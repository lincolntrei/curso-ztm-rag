{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2fbe983-8e6f-484f-ad99-e99821ae3a4d",
   "metadata": {},
   "source": [
    "# Geração de texto básico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164b587-c1b1-41a4-8301-2da237209a72",
   "metadata": {},
   "source": [
    "De inicio, sera usado o GPT 2 porque:\n",
    "\n",
    "* É gratuito\n",
    "* Podemos usar a biblioteca transformers\n",
    "\n",
    "Obs: GPT 2 só entende inglês"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07beb5da-7d94-488c-85ed-7c092f034672",
   "metadata": {},
   "source": [
    "#### Instalação das bibliotecas\n",
    "\n",
    "##### Instalação do pytorch com suporte a gpu de cuda versão 12.8 (verificar cuda da placa de vídeo)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "##### Instalação do transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04869c18-b111-4372-b894-8f61a9e6dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando Bibliotecas\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe457378-d601-44c6-ae98-6d89da6a8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o tokenizer e o modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128b082c-6ae6-4ce8-b1d8-64439b701013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello boss ... I'm sorry, but I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding o prompt para coletar input ids\n",
    "prompt = \"Hello boss ...\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\") # pt = pytorch\n",
    "#input_ids # Visualização\n",
    "\n",
    "# Gerar texto usando o modelo\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "#outputs # Visualização\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2920c9d-3929-463e-b531-680cc9f5d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_texto_simples(prompt, model, tokenizer, max_length = 100):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids, max_length = max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347db715-f948-4690-b2c9-a976e2ac76a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my friend ... I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to\n"
     ]
    }
   ],
   "source": [
    "# Testando a função\n",
    "prompt = \"Hello my friend ...\"\n",
    "texto_gerado = gerar_texto_simples(prompt,\n",
    "                                  model,\n",
    "                                  tokenizer,\n",
    "                                  max_length = 100)\n",
    "print(texto_gerado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74088aa2-b04d-4f13-a24f-45ba177253ec",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36794129-10b9-4a84-a272-6a4408d3cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = [\n",
    "    \"Machine Learning é uma área da inteligência artificial que permite que os computadores aprendam e tomem decisões sem serem explicitamente programados. O aprendizado é feito a partir de dados e o desempenho melhora com o tempo conforme novos dados são apresentados ao modelo.\",\n",
    "    \"Existem três tipos principais de aprendizado de máquina: supervisionado, não supervisionado e por reforço. No aprendizado supervisionado, o modelo é treinado com dados rotulados, enquanto no não supervisionado, o modelo tenta encontrar padrões ocultos nos dados sem rótulos.\",\n",
    "    \"Uma das aplicações mais conhecidas de Machine Learning é a classificação de e-mails como spam ou não spam. Isso é feito treinando um modelo com um grande conjunto de e-mails previamente classificados, permitindo que ele aprenda a distinguir padrões associados a cada categoria.\",\n",
    "    \"Redes neurais artificiais são modelos inspirados no funcionamento do cérebro humano e são muito utilizadas em tarefas como reconhecimento de imagens, processamento de linguagem natural e tradução automática. Elas são compostas por camadas de neurônios artificiais que processam as informações de forma hierárquica.\",\n",
    "    \"O treinamento de modelos de Machine Learning envolve a escolha de uma métrica de avaliação, como acurácia, precisão, recall ou F1-score, e a otimização de parâmetros para minimizar o erro da previsão. Técnicas como validação cruzada ajudam a evitar o overfitting.\",\n",
    "    \"Machine Learning é amplamente usado em sistemas de recomendação, como os que vemos na Netflix ou Amazon, onde os algoritmos analisam o histórico de comportamento dos usuários para sugerir novos produtos ou conteúdos que sejam relevantes para cada perfil individual.\",\n",
    "    \"Uma técnica muito usada para reduzir a dimensionalidade dos dados é a Análise de Componentes Principais (PCA), que transforma um conjunto de variáveis possivelmente correlacionadas em um conjunto menor de variáveis independentes chamadas de componentes principais.\",\n",
    "    \"Modelos de aprendizado de máquina podem sofrer com o problema de overfitting, quando aprendem muito bem os dados de treino, mas não conseguem generalizar para dados novos. Uma das soluções para esse problema é o uso de regularização e aumento da quantidade de dados de treinamento.\",\n",
    "    \"O pré-processamento de dados é uma etapa crucial em qualquer projeto de Machine Learning. Isso inclui tarefas como limpeza de dados, tratamento de valores ausentes, normalização e codificação de variáveis categóricas, garantindo que os dados estejam em um formato adequado para o treinamento do modelo.\",\n",
    "    \"Com o crescimento de bibliotecas como Scikit-learn, TensorFlow e PyTorch, o desenvolvimento de modelos de Machine Learning se tornou mais acessível a desenvolvedores e cientistas de dados, permitindo a criação de soluções robustas em áreas como finanças, saúde, marketing e segurança.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a184b5c4-6066-44fe-9bde-faa1b11b6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"Machine Learning is a field of artificial intelligence that enables computers to learn and make decisions without being explicitly programmed. Learning is done from data, and performance improves over time as new data is presented to the model.\",\n",
    "    \"There are three main types of machine learning: supervised, unsupervised, and reinforcement learning. In supervised learning, the model is trained with labeled data, while in unsupervised learning, the model tries to find hidden patterns in unlabeled data.\",\n",
    "    \"One of the most well-known applications of Machine Learning is classifying emails as spam or not spam. This is done by training a model with a large set of previously classified emails, allowing it to learn patterns associated with each category.\",\n",
    "    \"Artificial neural networks are models inspired by the functioning of the human brain and are widely used in tasks such as image recognition, natural language processing, and machine translation. They consist of layers of artificial neurons that process information hierarchically.\",\n",
    "    \"Training Machine Learning models involves choosing an evaluation metric, such as accuracy, precision, recall, or F1-score, and optimizing parameters to minimize prediction error. Techniques like cross-validation help avoid overfitting.\",\n",
    "    \"Machine Learning is widely used in recommendation systems, such as those seen on Netflix or Amazon, where algorithms analyze users' behavior history to suggest new products or content that is relevant to each individual profile.\",\n",
    "    \"A widely used technique to reduce data dimensionality is Principal Component Analysis (PCA), which transforms a set of possibly correlated variables into a smaller set of independent variables called principal components.\",\n",
    "    \"Machine learning models can suffer from the problem of overfitting, where they learn the training data too well but fail to generalize to new data. One solution to this problem is the use of regularization and increasing the amount of training data.\",\n",
    "    \"Data preprocessing is a crucial step in any Machine Learning project. This includes tasks such as data cleaning, handling missing values, normalization, and encoding categorical variables, ensuring that the data is in a suitable format for model training.\",\n",
    "    \"With the growth of libraries like Scikit-learn, TensorFlow, and PyTorch, the development of Machine Learning models has become more accessible to developers and data scientists, allowing the creation of robust solutions in areas such as finance, healthcare, marketing, and security.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc3ed59a-4446-4219-a924-80b63eefbe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenização\n",
    "# Todos os inputs precisam ter o mesmo tamanho (length)\n",
    "# Add um dummy token no final que sera ignorado pelo sistema\n",
    "# Ter o mesmo length => Isso é chamado de padding\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caeac024-cc73-4451-af8d-05ee8231f6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[37573, 18252,   318,   257,  2214,   286, 11666,  4430,   326, 13536,\n",
       "           9061,   284,  2193,   290,   787,  5370,  1231,   852, 11777, 27402,\n",
       "             13, 18252,   318,  1760,   422,  1366,    11,   290,  2854, 19575,\n",
       "            625,   640,   355,   649,  1366,   318,  5545,   284,   262,  2746,\n",
       "             13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 1858,   389,  1115,  1388,  3858,   286,  4572,  4673,    25, 28679,\n",
       "             11,   555, 16668, 16149,    11,   290, 37414,  4673,    13,   554,\n",
       "          28679,  4673,    11,   262,  2746,   318,  8776,   351, 15494,  1366,\n",
       "             11,   981,   287,   555, 16668, 16149,  4673,    11,   262,  2746,\n",
       "           8404,   284,  1064,  7104,  7572,   287,  9642,  9608,   276,  1366,\n",
       "             13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizando os dados\n",
    "dados_tokenizados = [tokenizer.encode_plus(\n",
    "    sentenca,\n",
    "    add_special_tokens = True, #special tokens = pontuação e etc\n",
    "    return_tensors = \"pt\", #pt = pytorch\n",
    "    padding = \"max_length\",\n",
    "    max_length = 60, #suficiente para caber todas as strings do projeto mas não sobrar muito\n",
    ") for sentenca in data]\n",
    "dados_tokenizados[:2] # exibindo apenas primeiros 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "564e1503-b89a-4195-94c3-7e8ae5004b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolando os input ids e os attention mask\n",
    "input_ids = [item['input_ids'].squeeze() for item in dados_tokenizados]\n",
    "attention_masks = [item['attention_mask'].squeeze() for item in dados_tokenizados]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176cd385-16b9-4427-a9dc-d25501d4f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter os input ids e os attention masks para tensors\n",
    "# esse passo é necessário para processar o tuned model\n",
    "input_ids = torch.stack(input_ids)\n",
    "attention_masks = torch.stack(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8142e57b-c5d6-4995-af79-4bea2b90bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding todas as sequencias para se certificar que tenham o mesmo length / necessário\n",
    "padded_input_ids = pad_sequence(input_ids,\n",
    "                                batch_first = True,\n",
    "                                padding_value = tokenizer.eos_token_id)\n",
    "padded_attention_masks = pad_sequence(attention_masks,\n",
    "                                     batch_first = True,\n",
    "                                     padding_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd24ed3-c27d-493e-86e6-2421669a8aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[37573, 18252,   318,   257,  2214,   286, 11666,  4430,   326, 13536,\n",
       "           9061,   284,  2193,   290,   787,  5370,  1231,   852, 11777, 27402,\n",
       "             13, 18252,   318,  1760,   422,  1366,    11,   290,  2854, 19575,\n",
       "            625,   640,   355,   649,  1366,   318,  5545,   284,   262,  2746,\n",
       "             13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 1858,   389,  1115,  1388,  3858,   286,  4572,  4673,    25, 28679,\n",
       "             11,   555, 16668, 16149,    11,   290, 37414,  4673,    13,   554,\n",
       "          28679,  4673,    11,   262,  2746,   318,  8776,   351, 15494,  1366,\n",
       "             11,   981,   287,   555, 16668, 16149,  4673,    11,   262,  2746,\n",
       "           8404,   284,  1064,  7104,  7572,   287,  9642,  9608,   276,  1366,\n",
       "             13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[37573, 18252,   318,   257,  2214,   286, 11666,  4430,   326, 13536,\n",
       "           9061,   284,  2193,   290,   787,  5370,  1231,   852, 11777, 27402,\n",
       "             13, 18252,   318,  1760,   422,  1366,    11,   290,  2854, 19575,\n",
       "            625,   640,   355,   649,  1366,   318,  5545,   284,   262,  2746,\n",
       "             13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 1858,   389,  1115,  1388,  3858,   286,  4572,  4673,    25, 28679,\n",
       "             11,   555, 16668, 16149,    11,   290, 37414,  4673,    13,   554,\n",
       "          28679,  4673,    11,   262,  2746,   318,  8776,   351, 15494,  1366,\n",
       "             11,   981,   287,   555, 16668, 16149,  4673,    11,   262,  2746,\n",
       "           8404,   284,  1064,  7104,  7572,   287,  9642,  9608,   276,  1366,\n",
       "             13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando uma classe dataset customizada incluindo datalabels\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = input_ids.clone()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids' : self.input_ids[idx],\n",
    "            'attention_mask' : self.attention_masks[idx],\n",
    "            'labels' : self.labels[idx]\n",
    "        }\n",
    "\n",
    "# criando objeto da classe\n",
    "dataset = TextDataset(padded_input_ids, padded_attention_masks)\n",
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011fa1bb-52c2-46d7-a9f7-b364aff64b0b",
   "metadata": {},
   "source": [
    "## Fine Tuning o GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51d6290c-2c37-4247-87c2-2ecf8ec64df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando os dados em batches\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8552df83-777d-4fd3-9fd4-20feefd9ed62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2.90610671043396\n",
      "Epoch 2 - Loss: 2.357048988342285\n",
      "Epoch 3 - Loss: 1.9948416948318481\n",
      "Epoch 4 - Loss: 1.6582231521606445\n",
      "Epoch 5 - Loss: 1.344558596611023\n",
      "Epoch 6 - Loss: 0.9374242424964905\n",
      "Epoch 7 - Loss: 0.7848874926567078\n",
      "Epoch 8 - Loss: 0.4702298045158386\n",
      "Epoch 9 - Loss: 0.4030720591545105\n",
      "Epoch 10 - Loss: 0.4024461507797241\n"
     ]
    }
   ],
   "source": [
    "# Inicializando um optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5)\n",
    "\n",
    "# Colocando o modelo em training mode\n",
    "model.train()\n",
    "\n",
    "# Loop de treino\n",
    "\n",
    "# vezes que o modelo sera treinado\n",
    "for epoch in range(10): \n",
    "    for batch in dataloader:\n",
    "        # Pegando os input ids e attention mask\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        # Resetando os gradients para zero / zerando o optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        # Processando o input e attention masks\n",
    "        outputs = model(input_ids = input_ids,\n",
    "                       attention_mask = attention_mask,\n",
    "                       labels = input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass: compute the gradients of the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Atualiza os parametros do modelo\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Printando o loss para a epoch atual para monitorar o progresso\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02b4d91a-b7b6-4e68-a78c-eba523dcb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo função para gerar texto\n",
    "def gerar_texto(prompt, model, tokenizer, max_length = 100):\n",
    "    # Encoding the prompt to get the input ids\n",
    "    inputs = tokenizer.encode_plus(prompt, return_tensors = \"pt\")\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Gerar texto usando o modelo\n",
    "    outputs = model.generate(input_ids, \n",
    "                             attention_mask = attention_mask, \n",
    "                             max_length = max_length)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85021106-4ec7-41b7-8a11-52370dd831c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About Machine Learning, we  learn algorithms by training data, allowing each model with a large set of previously unstructured data.\n"
     ]
    }
   ],
   "source": [
    "# Testando a função\n",
    "prompt = \"About Machine Learning, we \"\n",
    "texto_gerado = gerar_texto(prompt, model, tokenizer, max_length = 500)\n",
    "print(texto_gerado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20043448-b335-4288-8a16-c1ef3211c448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
