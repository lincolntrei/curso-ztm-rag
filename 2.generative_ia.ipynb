{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2fbe983-8e6f-484f-ad99-e99821ae3a4d",
   "metadata": {},
   "source": [
    "# Geração de texto básico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164b587-c1b1-41a4-8301-2da237209a72",
   "metadata": {},
   "source": [
    "De inicio, sera usado o GPT 2 porque:\n",
    "\n",
    "* É gratuito\n",
    "* Podemos usar a biblioteca transformers\n",
    "\n",
    "Obs: GPT 2 só entende inglês"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07beb5da-7d94-488c-85ed-7c092f034672",
   "metadata": {},
   "source": [
    "#### Instalação das bibliotecas\n",
    "\n",
    "##### Instalação do pytorch com suporte a gpu de cuda versão 12.8 (verificar cuda da placa de vídeo)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "##### Instalação do transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04869c18-b111-4372-b894-8f61a9e6dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando Bibliotecas\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe457378-d601-44c6-ae98-6d89da6a8d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a419236f394ffea2dc79ef89cde5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf6e32ac74142e5a28d123d364ee828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275910d5bb724f7794d56109384682b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1430ac076108431ebae57ca8d973e2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651f80dcf2214dcdb87fcd60ad36c6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d2f7ff34f0405ca752c6e656b859e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec75e2d7479741e493fc0bcb6dfea6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inicializando o tokenizer e o modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "128b082c-6ae6-4ce8-b1d8-64439b701013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello boss ... I'm sorry, but I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure if you're ready to go. I'm not sure\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding o prompt para coletar input ids\n",
    "prompt = \"Hello boss ...\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\") # pt = pytorch\n",
    "#input_ids # Visualização\n",
    "\n",
    "# Gerar texto usando o modelo\n",
    "outputs = model.generate(input_ids, max_length = 100)\n",
    "#outputs # Visualização\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2920c9d-3929-463e-b531-680cc9f5d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_texto_simples(prompt, model, tokenizer, max_length = 100):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids, max_length = max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "347db715-f948-4690-b2c9-a976e2ac76a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my friend ... I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to do this. I'm not going to be able to\n"
     ]
    }
   ],
   "source": [
    "# Testando a função\n",
    "prompt = \"Hello my friend ...\"\n",
    "texto_gerado = gerar_texto_simples(prompt,\n",
    "                                  model,\n",
    "                                  tokenizer,\n",
    "                                  max_length = 100)\n",
    "print(texto_gerado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36794129-10b9-4a84-a272-6a4408d3cc93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
